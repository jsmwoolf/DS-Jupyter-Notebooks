{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In machine learning, the Iris dataset is the most well known dataset used to measure the efficiency of algorithms.  The dataset was introduced in a 1936 paper by Ronald Fisher.\n",
    "\n",
    "We will perform a comparison of three algorithms:\n",
    "\n",
    "* Gaussian Naive Bayes\n",
    "* Logisitc Regression\n",
    "* Decision Tree\n",
    "* Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "\n",
    "We will read in the data from the .data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "f = open(\"iris.data\",\"r\")\n",
    "\n",
    "# Here, we want to omit the last two elements since they're \n",
    "# just empty space\n",
    "tempData = f.read().split('\\n')[:-2]\n",
    "tempData = [i.split(',') for i in tempData]\n",
    "\n",
    "d = pd.DataFrame(tempData, columns=['sepal length','sepal width',\n",
    "                                    'petal length','petal width','class'])\n",
    "\n",
    "d.head()\n",
    "\n",
    "X = d[['sepal length','sepal width','petal length','petal width']]\n",
    "Y = d['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "We'll first use Gaussian Naive Bayes to determine how well the algorithm can classify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbModel = GaussianNB()\n",
    "nbModel.fit(X,Y)\n",
    "\n",
    "(nbModel.predict(X) == Y).sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now, we'll use Logistic Regression with the following function:\n",
    "\n",
    "$$ g(x) = \\frac{1}{1+e^{-h(x)}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ h(x) = b_0 + b_1x_1 + ... + b_nx_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrModel = LogisticRegression()\n",
    "lrModel.fit(X,Y)\n",
    "\n",
    "(lrModel.predict(X) == Y).sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Next up, we'll use a decision tree to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dtModel = tree.DecisionTreeClassifier()\n",
    "dtModel.fit(X,Y)\n",
    "\n",
    "(dtModel.predict(X) == Y).sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Finally, we'll use an SVM to determine how accurate the model can classify the plants.  We'll be using support vector clustering as the kernel type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98666666666666669"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svmModel = SVC()\n",
    "svmModel.fit(X,Y)\n",
    "\n",
    "(svmModel.predict(X) == Y).sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking\n",
    "\n",
    "1. Decision Trees\n",
    "2. Support Vector Machine\n",
    "3. Tie between Gaussian Naive Bayes and Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
